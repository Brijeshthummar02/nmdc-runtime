{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Migrate MongoDB database from `nmdc-schema` `v10.2.0` to `v11.0.0`\n",
    "\n",
    "- TODO: Update the initial schema version in the heading, elsewhere in the notebook, and in the filename, to `v10.5.6`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31d85d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is special. Unlike all previous notebooks, each of which only used a single migrator; this notebook will be using multiple migrators.\n",
    "\n",
    "This notebook will be used to migrate the database from `v10.2.0` to `v11.0.0` (i.e. the initial version of the Berkeley schema).\n",
    "\n",
    "- TODO: In reality, it may be used to migrate the database from `v10.5.6` (released June 25, 2024), or any other `nmdc-schema` release that happens between now and when we switch over to the Berkeley schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ad4ab",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d358ba",
   "metadata": {},
   "source": [
    "### 1. Determine MongoDB collections involved.\n",
    "\n",
    "To determine this, we look at the migrator, itself (it's currently in https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_10_2_0_to_11_0_0.py). We make note of which collections are referenced by that migrator (whether for reading or for writing) and add them to the `COLLECTION_NAMES` list below.\n",
    "\n",
    "```py\n",
    "# TODO: Consider separating them into two lists: `COLLECTIONS_TO_DUMP` and `COLLECTIONS_TO_RESTORE`. Or, make a list of collections that I will manually delete from the origin server after running this notebook.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09966b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_unknown.py\n",
    "from_X_to_unknown = [\n",
    "    \"omics_processing_set\",\n",
    "    \"pooling_set\",\n",
    "    \"library_preparation_set\",\n",
    "    \"extraction_set\"\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR23.py\n",
    "from_10_2_0_to_PR23 = [\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"mags_activity_set\",\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"metaproteomics_analysis_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR4.py\n",
    "from_PR23_to_PR4 = [\n",
    "    \"omics_processing_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR53.py\n",
    "from_PR4_to_PR53 = [\n",
    "    \"omics_processing_set\",\n",
    "    \"biosample_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR21.py\n",
    "from_PR53_to_PR21 = [\n",
    "    \"study_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR129.py\n",
    "from_PR21_to_PR129 = [\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR31.py\n",
    "from_PR129_to_PR31 = [\n",
    "    \"mags_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"omics_processing_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\"\n",
    "    \"metaproteomics_analysis_activity_set\"\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR9.py\n",
    "from_PR31_to_PR9 = [\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"mags_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"metaproteomics_analysis_activity_set\",\n",
    "\n",
    "    \"omics_processing_set\",\n",
    "    \"workflow_chain_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR19_and_PR70.py\n",
    "from_PR9_to_PR19_PR70 = [\n",
    "    \"instrument_set\",\n",
    "    \"omics_processing_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR2_and_PR24.py\n",
    "from_PR19_PR70_to_PR2_PR24 = [\n",
    "    \"omics_processing_set\", \n",
    "    \"data_generation_set\",\n",
    "\n",
    "    \"mags_activity_set\", \n",
    "    \"mags_set\",\n",
    "    \n",
    "    \"metabolomics_analysis_activity_set\", \n",
    "    \"metabolomics_analysis_set\",\n",
    "    \n",
    "    \"metagenome_annotation_activity_set\", \n",
    "    \"metagenome_annotation_set\",\n",
    "    \n",
    "    \"metagenome_sequencing_activity_set\", \n",
    "    \"metagenome_sequencing_set\",\n",
    "    \n",
    "    \"metaproteomics_analysis_activity_set\", \n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"nom_analysis_set\",\n",
    "    \n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "    \n",
    "    \"activity_set\",\n",
    "    \"workflow_execution_set\"    \n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR10.py\n",
    "from_PR2_PR24_to_PR10 = [\n",
    "    \"biosample_set\",\n",
    "    \"data_object_set\",\n",
    "    \"functional_annotation_agg\",\n",
    "    \"study_set\",\n",
    "    \"extraction_set\",\n",
    "    \"field_research_site_set\",\n",
    "    \"library_preparation_set\",\n",
    "    \"mags_set\",\n",
    "    \"metabolomics_analysis_set\",\n",
    "    \"metagenome_annotation_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_sequencing_set\",\n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \"nom_analysis_set\",\n",
    "    \"data_generation_set\",\n",
    "    \"pooling_set\",\n",
    "    \"processed_sample_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR3.py\n",
    "from_PR10_to_PR3 = [\n",
    "    \"data_generation_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR176.py\n",
    "from_X_to_PR176 = [\n",
    "    \"read_qc_analysis_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_PR176_to_PR104.py\n",
    "from_PR176_to_PR104 = [\n",
    "    \"data_generation_set\"\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR192.py\n",
    "from_X_to_PR192 = [\n",
    "    \"extraction_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/partials/migrator_from_10_2_0_to_11_0_0/migrator_from_X_to_PR104.py\n",
    "from_X_to_PR104 = [\n",
    "    \"workflow_execution_set\",\n",
    "    \"metagenome_annotation_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metatranscriptome_assembly_set\",\n",
    "    \"metatranscriptome_annotation_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \"mags_set\",\n",
    "    \"metagenome_sequencing_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \"metabolomics_analysis_set\",\n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \"nom_analysis_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_PR104_to_PR195.py\n",
    "from_PR104_to_PR195 = [\n",
    "    \"collecting_biosamples_from_site_set\",\n",
    "    \"protocol_execution_set\",\n",
    "    \"storage_process_set\",\n",
    "    \"material_processing_set\",\n",
    "    \"pooling_set\",\n",
    "    \"extraction_set\",\n",
    "    \"library_preparation_set\",\n",
    "    \"sub_sampling_process_set\",\n",
    "    \"mixing_process_set\",\n",
    "    \"filtration_process_set\",\n",
    "    \"chromatographic_separation_process_set\",\n",
    "    \"dissolving_process_set\",\n",
    "    \"chemical_conversion_process_set\",\n",
    "    \"data_generation_set\",\n",
    "    \"nucleotide_sequencing_set\",\n",
    "    \"mass_spectrometry_set\",\n",
    "    \"workflow_chain_set\",\n",
    "    \"workflow_execution_set\",\n",
    "    \"metagenome_annotation_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metatranscriptome_assembly_set\",\n",
    "    \"metatranscriptome_annotation_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \"mags_set\",\n",
    "    \"metagenome_sequencing_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \"metabolomics_analysis_set\",\n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \"nom_analysis_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_PR195_to_unknown.py\n",
    "from_PR195_to_unknown = [\n",
    "    \"workflow_execution_set\",\n",
    "    \"workflow_chain_set\",\n",
    "]\n",
    "\n",
    "# Note: `*arr` in Python is like `...arr` in JavaScript (it's a \"spread\" operator).\n",
    "COLLECTION_NAMES: list[str] = [\n",
    "    *from_X_to_unknown,\n",
    "    *from_10_2_0_to_PR23,\n",
    "    *from_PR23_to_PR4,\n",
    "    *from_PR4_to_PR53,\n",
    "    *from_PR53_to_PR21,\n",
    "    *from_PR21_to_PR129,\n",
    "    *from_PR129_to_PR31,\n",
    "    *from_PR31_to_PR9,\n",
    "    *from_PR9_to_PR19_PR70,\n",
    "    *from_PR19_PR70_to_PR2_PR24,\n",
    "    *from_PR2_PR24_to_PR10,\n",
    "    *from_PR10_to_PR3,\n",
    "    *from_X_to_PR176,\n",
    "    *from_PR176_to_PR104,\n",
    "    *from_X_to_PR192,\n",
    "    *from_X_to_PR104,\n",
    "    *from_PR104_to_PR195,\n",
    "    *from_PR195_to_unknown,\n",
    "]\n",
    "print(str(len(COLLECTION_NAMES)) + \" collection names\")\n",
    "\n",
    "# Eliminate duplicates.\n",
    "COLLECTION_NAMES = list(set(COLLECTION_NAMES))\n",
    "print(str(len(COLLECTION_NAMES)) + \" collection names (distinct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f351e8",
   "metadata": {},
   "source": [
    "### 2. Coordinate with stakeholders.\n",
    "\n",
    "We will be enacting full Runtime and Database downtime for this migration. Ensure stakeholders are aware of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a35c3",
   "metadata": {},
   "source": [
    "### 3. Set up environment.\n",
    "\n",
    "Here, you'll prepare an environment for running this notebook.\n",
    "\n",
    "1. Start a **MongoDB server** on your local machine (and ensure it does **not** already contain a database named `nmdc`).\n",
    "    1. You can start a [Docker](https://hub.docker.com/_/mongo)-based MongoDB server at `localhost:27055` by running this command (this MongoDB server will be accessible without a username or password).\n",
    "       ```shell\n",
    "       docker run --rm --detach --name mongo-migration-transformer -p 27055:27017 mongo:6.0.4\n",
    "       ```\n",
    "2. Create and populate a **notebook configuration file** named `.notebook.env`.\n",
    "    1. You can use `.notebook.env.example` as a template:\n",
    "       ```shell\n",
    "       $ cp .notebook.env.example .notebook.env\n",
    "       ```\n",
    "3. Create and populate the two **MongoDB configuration files** that this notebook will use to connect to the \"origin\" and \"transformer\" MongoDB servers. The \"origin\" MongoDB server is the one that contains the database you want to migrate; and the \"transformer\" MongoDB server is the one you want to use to perform the data transformations. In practice, the \"origin\" MongoDB server is typically a remote server, and the \"transformer\" MongoDB server is typically a local server.\n",
    "    1. You can use `.mongo.yaml.example` as a template:\n",
    "       ```shell\n",
    "       $ cp .mongo.yaml.example .mongo.origin.yaml\n",
    "       $ cp .mongo.yaml.example .mongo.transformer.yaml\n",
    "       ```\n",
    "       > When populating the file for the origin MongoDB server, use credentials that have **both read and write access** to the `nmdc` database.\n",
    "\n",
    "- TODO: Be more specific about the Mongo privileges necessary to perform a `mongodump` and a `mongorestore` that may involve creating/deleting collections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69937b18",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81196a",
   "metadata": {},
   "source": [
    "### Install Python dependencies\n",
    "\n",
    "In this step, you'll [install](https://saturncloud.io/blog/what-is-the-difference-between-and-in-jupyter-notebooks/) the Python packages upon which this notebook depends.\n",
    "\n",
    "> Note: If the output of this cell says \"Note: you may need to restart the kernel to use updated packages\", restart the kernel (not the notebook cells) now.\n",
    "\n",
    "References: \n",
    "- https://pypi.org/project/nmdc-schema/\n",
    "- https://github.com/microbiomedata/berkeley-schema-fy24\n",
    "- How to `pip install` a Git branch: https://stackoverflow.com/a/20101940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a0af308c3185b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r requirements.txt\n",
    "%pip install nmdc-schema==11.0.0rc16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407c354",
   "metadata": {},
   "source": [
    "### Import Python dependencies\n",
    "\n",
    "Import the Python objects upon which this notebook depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party packages:\n",
    "import pymongo\n",
    "from jsonschema import Draft7Validator\n",
    "from nmdc_schema.nmdc_data import get_nmdc_jsonschema_dict, SchemaVariantIdentifier\n",
    "from nmdc_schema.migrators.adapters.mongo_adapter import MongoAdapter\n",
    "from nmdc_schema.migrators.migrator_from_10_2_0_to_11_0_0 import Migrator\n",
    "\n",
    "# First-party packages:\n",
    "from helpers import Config\n",
    "from bookkeeper import Bookkeeper, MigrationEvent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b20ff4",
   "metadata": {},
   "source": [
    "### Parse configuration files\n",
    "\n",
    "Parse the notebook and Mongo configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "# Define some aliases we can use to make the shell commands in this notebook easier to read.\n",
    "mongodump = cfg.mongodump_path\n",
    "mongorestore = cfg.mongorestore_path\n",
    "\n",
    "# Perform a sanity test of the application paths.\n",
    "!{mongodump} --version\n",
    "!{mongorestore} --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68245d2b",
   "metadata": {},
   "source": [
    "### Create MongoDB clients\n",
    "\n",
    "Create MongoDB clients you can use to access the \"origin\" and \"transformer\" MongoDB servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mongo client for \"origin\" MongoDB server.\n",
    "origin_mongo_client = pymongo.MongoClient(host=cfg.origin_mongo_server_uri, directConnection=True)\n",
    "\n",
    "# Mongo client for \"transformer\" MongoDB server.\n",
    "transformer_mongo_client = pymongo.MongoClient(host=cfg.transformer_mongo_server_uri)\n",
    "\n",
    "# Perform sanity tests of those MongoDB clients' abilities to access their respective MongoDB servers.\n",
    "with pymongo.timeout(3):\n",
    "    # Display the MongoDB server version (running on the \"origin\" Mongo server).\n",
    "    print(\"Origin Mongo server version:      \" + origin_mongo_client.server_info()[\"version\"])\n",
    "\n",
    "    # Sanity test: Ensure the origin database exists.\n",
    "    assert \"nmdc\" in origin_mongo_client.list_database_names(), \"Origin database does not exist.\"\n",
    "\n",
    "    # Display the MongoDB server version (running on the \"transformer\" Mongo server).\n",
    "    print(\"Transformer Mongo server version: \" + transformer_mongo_client.server_info()[\"version\"])\n",
    "\n",
    "    # Sanity test: Ensure the transformation database does not exist.\n",
    "    assert \"nmdc\" not in transformer_mongo_client.list_database_names(), \"Transformation database already exists.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc387abc62686091",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create JSON Schema validator\n",
    "\n",
    "In this step, you'll create a JSON Schema validator for the NMDC Schema.\n",
    "\n",
    "- TODO: Consider whether the JSON Schema validator version is consistent with the JSON Schema version (e.g. draft 7 versus draft 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c982eb0c04e606d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmdc_jsonschema: dict = get_nmdc_jsonschema_dict(variant=SchemaVariantIdentifier.nmdc_materialized_patterns)\n",
    "nmdc_jsonschema_validator = Draft7Validator(nmdc_jsonschema)\n",
    "\n",
    "# Perform sanity tests of the NMDC Schema dictionary and the JSON Schema validator.\n",
    "# Reference: https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/protocols/#jsonschema.protocols.Validator.check_schema\n",
    "print(\"NMDC Schema title:   \" + nmdc_jsonschema[\"title\"])\n",
    "print(\"NMDC Schema version: \" + nmdc_jsonschema[\"version\"])\n",
    "\n",
    "nmdc_jsonschema_validator.check_schema(nmdc_jsonschema)  # raises exception if schema is invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975ac24",
   "metadata": {},
   "source": [
    "### TODO: Revoke write access to the \"origin\" MongoDB server\n",
    "\n",
    "This is so people don't make changes to the original data while the migration is happening, given that the migration ends with an overwriting of the original data.\n",
    "\n",
    "Note: The migrator Mongo user may need additional permissions in order to manipulate Mongo user roles to the extent necessary to accomplish this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c8935",
   "metadata": {},
   "source": [
    "### Delete obsolete dumps\n",
    "\n",
    "Delete any existing dumps so that the dumps you generate below will not be mixed in with any unrelated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {cfg.origin_dump_folder_path}\n",
    "!rm -rf {cfg.transformer_dump_folder_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4994a0",
   "metadata": {},
   "source": [
    "### Dump collections from the \"origin\" MongoDB server\n",
    "\n",
    "Use `mongodump` to dump the collections involved in this migration **from** the \"origin\" MongoDB server **into** a local directory.\n",
    "\n",
    "> Since `mongodump` doesn't provide a CLI option we can use to specify the collections we _want_ the dump to include, we use multiple occurrences of the `--excludeCollection` CLI option to exclude each collection we do _not_ want the dump to include. The end result is the same—there's just that extra step involved.\n",
    "\n",
    "- TODO: Consider ensuring that the local dump target folder is empty before doing this dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ac241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a string containing zero or more `--excludeCollection=\"...\"` options, which can be included in a `mongodump` command.\n",
    "all_collection_names: list[str] = origin_mongo_client[\"nmdc\"].list_collection_names()\n",
    "non_agenda_collection_names = [name for name in all_collection_names if name not in COLLECTION_NAMES]\n",
    "exclusion_options = [f\"--excludeCollection='{name}'\" for name in non_agenda_collection_names]\n",
    "exclusion_options_str = \" \".join(exclusion_options)  # separates each option with a space\n",
    "print(exclusion_options_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Temporarily excluding nothing so that everything gets dumped!\n",
    "exclusion_options_str = \"\"\n",
    "\n",
    "# Dump the not-excluded collections from the \"origin\" database.\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{cfg.origin_dump_folder_path}\" \\\n",
    "  {exclusion_options_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4994a0",
   "metadata": {},
   "source": [
    "### Load the dumped collections into the \"transformer\" MongoDB server\n",
    "\n",
    "Use `mongorestore` to load the dumped collections **from** the local directory **into** the \"transformer\" MongoDB server.\n",
    "\n",
    "> Since it's possible that the dump included extra collections (due to someone having created a collection between the time you generated the `--excludeCollection` CLI options and the time you ran `mongodump` above), we will use the `--nsInclude` CLI option to indicate which specific collections—from the dump—we want to load into the \"transformer\" database.\n",
    "\n",
    "> Note: This step typically takes 3 minutes (on a MacBook Pro M1, when running MongoDB in a Docker container).\n",
    "\n",
    "- TODO: Are \"views\" included in `mongodump` dumps? If so, how does `mongorestore` handle them—does it restore them as \"views\" or as normal \"collections\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a string containing zero or more `--nsInclude=\"...\"` options, which can be included in a `mongorestore` command.\n",
    "inclusion_options = [f\"--nsInclude='nmdc.{name}'\" for name in COLLECTION_NAMES]\n",
    "inclusion_options_str = \" \".join(inclusion_options)  # separates each option with a space\n",
    "print(inclusion_options_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Temporarily include nothing explicitly so that everything gets restored!\n",
    "inclusion_options_str = \"\"\n",
    "\n",
    "# Restore the dumped collections to the \"transformer\" MongoDB server.\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --drop \\\n",
    "  --preserveUUID \\\n",
    "  --stopOnError \\\n",
    "  --dir=\"{cfg.origin_dump_folder_path}\" \\\n",
    "  {inclusion_options_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3c9c4",
   "metadata": {},
   "source": [
    "### Transform the collections within the \"transformer\" MongoDB server\n",
    "\n",
    "Use the migrator to transform the collections in the \"transformer\" database.\n",
    "\n",
    "> Reminder: The database transformation functions are defined in the `nmdc-schema` Python package installed earlier.\n",
    "\n",
    "> Reminder: The \"origin\" database is **not** affected by this step.\n",
    "\n",
    "- TODO: Consider deleting the existing log or appending a timestamp to the log filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ee3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup a logger that writes to a file.\n",
    "# TODO: Move this logger stuff to `helpers.py`.`\n",
    "LOG_FILE_PATH = \"./tmp.log\"\n",
    "logger = logging.getLogger(name=\"migrator_logger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "file_handler = logging.FileHandler(LOG_FILE_PATH)\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\",\n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "file_handler.setFormatter(formatter)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()  # avoid duplicate log entries\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05869340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a MongoAdapter bound to the \"transformer\" database.\n",
    "adapter = MongoAdapter(\n",
    "    database=transformer_mongo_client[\"nmdc\"],\n",
    "    on_collection_created=lambda name: print(f'Created collection \"{name}\"'),\n",
    "    on_collection_renamed=lambda old_name, name: print(f'Renamed collection \"{old_name}\" to \"{name}\"'),\n",
    "    on_collection_deleted=lambda name: print(f'Deleted collection \"{name}\"'),\n",
    ")\n",
    "\n",
    "# Instantiate a Migrator bound to that adapter.\n",
    "migrator = Migrator(adapter=adapter, logger=logger)\n",
    "\n",
    "# Execute the Migrator's `upgrade` method to perform the migration.\n",
    "migrator.upgrade()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c090068",
   "metadata": {},
   "source": [
    "### Validate the transformed documents\n",
    "\n",
    "Now that we have transformed the database, validate each document in each collection in the \"transformer\" MongoDB server.\n",
    "\n",
    "> Reference: https://github.com/microbiomedata/nmdc-runtime/blob/main/metadata-translation/src/bin/validate_json.py\n",
    "\n",
    "- TODO: Consider validating the (large) `functional_annotation_agg` collection _last_ so we find out about validation errors, if any, in _other_ (smaller) collections sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05869340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that, if the (large) \"functional_annotation_agg\" collection is present in `COLLECTION_NAMES`,\n",
    "# it goes at the end of the list we process. That way, we can find out about validation errors in\n",
    "# other collections without having to wait for that (large) collection to be validated before them.\n",
    "ordered_collection_names = sorted(COLLECTION_NAMES.copy())\n",
    "large_collection_name = \"functional_annotation_agg\"\n",
    "if large_collection_name in ordered_collection_names:\n",
    "    ordered_collection_names = list(filter(lambda n: n != large_collection_name, ordered_collection_names))\n",
    "    ordered_collection_names.append(large_collection_name)\n",
    "\n",
    "# TODO: Only validate documents in the collections that we will be restoring.\n",
    "#       Note: If a collection listed in `COLLECTION_NAMES` doesn't exist in the transformation\n",
    "#             database anymore, the inner `for` loop will just have zero iterations.\n",
    "for collection_name in ordered_collection_names:\n",
    "    \n",
    "    # FIXME: Temporarily skip collections I know are invalid (so I can test the others)!\n",
    "    if collection_name in [\"data_object_set\", \"workflow_execution_set\", \"functional_annotation_agg\"]:\n",
    "        continue\n",
    "\n",
    "    collection = transformer_mongo_client[\"nmdc\"][collection_name]\n",
    "    num_documents_in_collection = collection.count_documents({})\n",
    "    print(f\"Validating collection {collection_name} ({num_documents_in_collection} documents)\")\n",
    "\n",
    "    for document in collection.find():\n",
    "        # Validate the transformed document.\n",
    "        #\n",
    "        # Reference: https://github.com/microbiomedata/nmdc-schema/blob/main/src/docs/schema-validation.md\n",
    "        #\n",
    "        # Note: Dictionaries originating as Mongo documents include a Mongo-generated key named `_id`. However,\n",
    "        #       the NMDC Schema does not describe that key and, indeed, data validators consider dictionaries\n",
    "        #       containing that key to be invalid with respect to the NMDC Schema. So, here, we validate a\n",
    "        #       copy (i.e. a shallow copy) of the document that lacks that specific key.\n",
    "        #\n",
    "        # Note: `root_to_validate` is a dictionary having the shape: { \"some_collection_name\": [ some_document ] }\n",
    "        #       Reference: https://docs.python.org/3/library/stdtypes.html#dict (see the \"type constructor\" section)\n",
    "        #\n",
    "        document_without_underscore_id_key = {key: value for key, value in document.items() if key != \"_id\"}\n",
    "        root_to_validate = dict([(collection_name, [document_without_underscore_id_key])])\n",
    "        nmdc_jsonschema_validator.validate(root_to_validate)  # raises exception if invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf77c7",
   "metadata": {},
   "source": [
    "### Dump the collections from the \"transformer\" MongoDB server\n",
    "\n",
    "Now that the collections have been transformed and validated, dump them **from** the \"transformer\" MongoDB server **into** a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the database from the \"transformer\" MongoDB server.\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{cfg.transformer_dump_folder_path}\" \\\n",
    "  {exclusion_options_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fcb281d9d3222",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create a bookkeeper\n",
    "\n",
    "Create a `Bookkeeper` that can be used to document migration events in the \"origin\" server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookkeeper = Bookkeeper(mongo_client=origin_mongo_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c8891",
   "metadata": {},
   "source": [
    "### Indicate — on the \"origin\" server — that the migration is underway\n",
    "\n",
    "Add an entry to the migration log collection to indicate that this migration has started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookkeeper.record_migration_event(migrator=migrator, event=MigrationEvent.MIGRATION_STARTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c253e6f",
   "metadata": {},
   "source": [
    "### TODO: Drop the original collections from the \"origin\" MongoDB server\n",
    "\n",
    "This is necessary for situations where collections were renamed or deleted. The `--drop` option of `mongorestore` only drops collections that exist in the dump. We may need `mongosh` for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bdc11",
   "metadata": {},
   "source": [
    "### Load the collections into the \"origin\" MongoDB server\n",
    "\n",
    "Load the transformed collections into the \"origin\" MongoDB server, **replacing** the collections there that have the same names.\n",
    "\n",
    "> Note: If the migration involved renaming or deleting a collection, the collection having the original name will continue to exist in the \"origin\" database until someone deletes it manually.\n",
    "\n",
    "> Estimated time when running on laptop: 17 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the same-named collection(s) on the origin server, with the transformed one(s).\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --verbose \\\n",
    "  --dir=\"{cfg.transformer_dump_folder_path}\" \\\n",
    "  --drop --preserveUUID \\\n",
    "  --stopOnError \\\n",
    "  {inclusion_options_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ee89a79148499",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Indicate that the migration is complete\n",
    "\n",
    "Add an entry to the migration log collection to indicate that this migration is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eaa6c92789c4f3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bookkeeper.record_migration_event(migrator=migrator, event=MigrationEvent.MIGRATION_COMPLETED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c856a8",
   "metadata": {},
   "source": [
    "### TODO: Reinstate write access to the MongoDB server\n",
    "\n",
    "This effectively un-does the access revocation that we did earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
