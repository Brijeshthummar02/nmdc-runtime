{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Migrate MongoDB database from `nmdc-schema` `v10.2.0` to `v11.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31d85d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is special. Unlike all previous notebooks, each of which only used a single migrator; this notebook will be using multiple migrators.\n",
    "\n",
    "This notebook will be used to migrate the database from `v10.2.0` (i.e. the final version of the pre-Berkeley schema) to `v11.0.0` (i.e. the initial version of the Berkeley schema)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ad4ab",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d358ba",
   "metadata": {},
   "source": [
    "### 1. Determine MongoDB collections involved.\n",
    "\n",
    "To determine this, we look at all the migrators listed in this \"meta issue\": https://github.com/microbiomedata/nmdc-schema/issues/1607. In each migrator, we make note of which collections are involved (whether for reading or for writing) and add them to the `COLLECTION_NAMES` list below.\n",
    "\n",
    "```py\n",
    "# TODO: Consider separating them into two lists: `COLLECTIONS_TO_DUMP` and `COLLECTIONS_TO_RESTORE`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "09966b0d",
   "metadata": {},
   "source": [
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_unknown.py\n",
    "from_X_to_unknown = [\n",
    "    \"omics_processing_set\",\n",
    "    \"pooling_set\",\n",
    "    \"library_preparation_set\",\n",
    "    \"extraction_set\"\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR23.py\n",
    "from_10_2_0_to_PR23 = [\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"mags_activity_set\",\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"metaproteomics_analysis_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR4.py\n",
    "from_PR23_to_PR4 = [\n",
    "    \"omics_processing_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR53.py\n",
    "from_PR4_to_PR53 = [\n",
    "    \"omics_processing_set\",\n",
    "    \"biosample_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR21.py\n",
    "from_PR53_to_PR21 = [\n",
    "    \"study_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR129.py\n",
    "from_PR21_to_PR129 = [\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR31.py\n",
    "from_PR129_to_PR31 = [\n",
    "    \"mags_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"omics_processing_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\"\n",
    "    \"metaproteomics_analysis_activity_set\"\n",
    "]\n",
    "\n",
    "# TODO: Ensure this accounts for the collection names in the _final_ version\n",
    "#       of the `nmdc_schema/migrators/migrator_from_X_to_PR9.py` migrator.\n",
    "#       See: https://github.com/microbiomedata/berkeley-schema-fy24/pull/127\n",
    "from_PR31_to_PR9 = [\n",
    "    \"metagenome_sequencing_activity_set\",\n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"metagenome_annotation_activity_set\",\n",
    "    \"mags_activity_set\",\n",
    "    \"metabolomics_analysis_activity_set\",\n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"metaproteomics_analysis_activity_set\",\n",
    "\n",
    "    \"omics_processing_set\",\n",
    "    \"workflow_chain_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR19_and_PR70.py\n",
    "from_PR9_to_PR19_PR70 = [\n",
    "    \"instrument_set\",\n",
    "    \"omics_processing_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR2_and_PR24.py\n",
    "from_PR19_PR70_to_PR2_PR24 = [\n",
    "    \"omics_processing_set\", \n",
    "    \"data_generation_set\",\n",
    "\n",
    "    \"mags_activity_set\", \n",
    "    \"mags_set\",\n",
    "    \n",
    "    \"metabolomics_analysis_activity_set\", \n",
    "    \"metabolomics_analysis_set\",\n",
    "    \n",
    "    \"metagenome_annotation_activity_set\", \n",
    "    \"metagenome_annotation_set\",\n",
    "    \n",
    "    \"metagenome_sequencing_activity_set\", \n",
    "    \"metagenome_sequencing_set\",\n",
    "    \n",
    "    \"metaproteomics_analysis_activity_set\", \n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \n",
    "    \"metatranscriptome_activity_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \n",
    "    \"nom_analysis_activity_set\",\n",
    "    \"nom_analysis_set\",\n",
    "    \n",
    "    \"read_based_taxonomy_analysis_activity_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \n",
    "    \"read_qc_analysis_activity_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "    \n",
    "    \"activity_set\",\n",
    "    \"workflow_execution_set\"    \n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR10.py\n",
    "from_PR2_PR24_to_PR10 = [\n",
    "    \"biosample_set\",\n",
    "    \"data_object_set\",\n",
    "    \"functional_annotation_agg\",\n",
    "    \"study_set\",\n",
    "    \"extraction_set\",\n",
    "    \"field_research_site_set\",\n",
    "    \"library_preparation_set\",\n",
    "    \"mags_set\",\n",
    "    \"metabolomics_analysis_set\",\n",
    "    \"metagenome_annotation_set\",\n",
    "    \"metagenome_assembly_set\",\n",
    "    \"metagenome_sequencing_set\",\n",
    "    \"metaproteomics_analysis_set\",\n",
    "    \"metatranscriptome_analysis_set\",\n",
    "    \"nom_analysis_set\",\n",
    "    \"data_generation_set\",\n",
    "    \"pooling_set\",\n",
    "    \"processed_sample_set\",\n",
    "    \"read_based_taxonomy_analysis_set\",\n",
    "    \"read_qc_analysis_set\",\n",
    "]\n",
    "\n",
    "# https://github.com/microbiomedata/berkeley-schema-fy24/blob/main/nmdc_schema/migrators/migrator_from_X_to_PR3.py\n",
    "from_PR10_to_PR3 = [\n",
    "    \"data_generation_set\",\n",
    "]\n",
    "\n",
    "# Note: `*arr` in Python is like `...arr` in JavaScript (it's a \"spread\" operator).\n",
    "COLLECTION_NAMES: list[str] = [\n",
    "    *from_X_to_unknown,\n",
    "    *from_10_2_0_to_PR23,\n",
    "    *from_PR23_to_PR4,\n",
    "    *from_PR4_to_PR53,\n",
    "    *from_PR53_to_PR21,\n",
    "    *from_PR21_to_PR129,\n",
    "    *from_PR129_to_PR31,\n",
    "    *from_PR31_to_PR9,\n",
    "    *from_PR9_to_PR19_PR70,\n",
    "    *from_PR19_PR70_to_PR2_PR24,\n",
    "    *from_PR2_PR24_to_PR10,\n",
    "    *from_PR10_to_PR3,\n",
    "]\n",
    "print(str(len(COLLECTION_NAMES)) + \" collection names\")\n",
    "\n",
    "# Eliminate duplicates.\n",
    "COLLECTION_NAMES = list(set(COLLECTION_NAMES))\n",
    "print(str(len(COLLECTION_NAMES)) + \" collection names (distinct)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17f351e8",
   "metadata": {},
   "source": [
    "### 2. Coordinate with stakeholders.\n",
    "\n",
    "We will be enacting full Runtime and Database downtime for this migration. Ensure stakeholders are aware of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a35c3",
   "metadata": {},
   "source": [
    "### 3. Set up environment.\n",
    "\n",
    "Here, you'll prepare an environment for running this notebook.\n",
    "\n",
    "1. Start a **MongoDB server** on your local machine (and ensure it does **not** already contain a database named `nmdc`).\n",
    "    1. You can start a [Docker](https://hub.docker.com/_/mongo)-based MongoDB server at `localhost:27055` by running this command (this MongoDB server will be accessible without a username or password).\n",
    "       ```shell\n",
    "       docker run --rm --detach --name mongo-migration-transformer -p 27055:27017 mongo:6.0.4\n",
    "       ```\n",
    "2. Create and populate a **notebook configuration file** named `.notebook.env`.\n",
    "    1. You can use `.notebook.env.example` as a template:\n",
    "       ```shell\n",
    "       $ cp .notebook.env.example .notebook.env\n",
    "       ```\n",
    "3. Create and populate the two **MongoDB configuration files** that this notebook will use to connect to the \"origin\" and \"transformer\" MongoDB servers. The \"origin\" MongoDB server is the one that contains the database you want to migrate; and the \"transformer\" MongoDB server is the one you want to use to perform the data transformations. In practice, the \"origin\" MongoDB server is typically a remote server, and the \"transformer\" MongoDB server is typically a local server.\n",
    "    1. You can use `.mongo.yaml.example` as a template:\n",
    "       ```shell\n",
    "       $ cp .mongo.yaml.example .mongo.origin.yaml\n",
    "       $ cp .mongo.yaml.example .mongo.transformer.yaml\n",
    "       ```\n",
    "       > When populating the file for the origin MongoDB server, use credentials that have **both read and write access** to the `nmdc` database.\n",
    "\n",
    "```py\n",
    "# TODO: Update the `.mongo.origin.yaml` file to use the `nmdc_migrator` user on the Berkeley Mongo server.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69937b18",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81196a",
   "metadata": {},
   "source": [
    "### Install Python dependencies\n",
    "\n",
    "In this step, you'll [install](https://saturncloud.io/blog/what-is-the-difference-between-and-in-jupyter-notebooks/) the Python packages upon which this notebook depends.\n",
    "\n",
    "> Note: If the output of this cell says \"Note: you may need to restart the kernel to use updated packages\", restart the kernel (not the notebook cells) now.\n",
    "\n",
    "References: \n",
    "- https://pypi.org/project/nmdc-schema/\n",
    "- https://github.com/microbiomedata/berkeley-schema-fy24\n",
    "- How to `pip install` a Git branch: https://stackoverflow.com/a/20101940"
   ]
  },
  {
   "cell_type": "code",
   "id": "e25a0af308c3185b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install nmdc-schema==11.0.0rc4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a407c354",
   "metadata": {},
   "source": [
    "### Import Python dependencies\n",
    "\n",
    "Import the Python objects upon which this notebook depends."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbecd561",
   "metadata": {},
   "source": [
    "# Third-party packages:\n",
    "import pymongo\n",
    "from jsonschema import Draft7Validator\n",
    "from nmdc_schema.nmdc_data import get_nmdc_jsonschema_dict\n",
    "from nmdc_schema.migrators.adapters.mongo_adapter import MongoAdapter\n",
    "\n",
    "from nmdc_schema.migrators.migrator_from_X_to_unknown import Migrator as M1\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR23 import Migrator as M2\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR4 import Migrator as M3\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR53 import Migrator as M4\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR21 import Migrator as M5\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR129 import Migrator as M6\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR31 import Migrator as M7\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR9 import Migrator as M8\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR19_and_PR70 import Migrator as M9\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR2_and_PR24 import Migrator as M10\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR10 import Migrator as M11\n",
    "from nmdc_schema.migrators.migrator_from_X_to_PR3 import Migrator as M12\n",
    "\n",
    "# First-party packages:\n",
    "from helpers import Config\n",
    "from bookkeeper import Bookkeeper, MigrationEvent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99b20ff4",
   "metadata": {},
   "source": [
    "### Parse configuration files\n",
    "\n",
    "Parse the notebook and Mongo configuration files."
   ]
  },
  {
   "cell_type": "code",
   "id": "1eac645a",
   "metadata": {},
   "source": [
    "cfg = Config()\n",
    "\n",
    "# Define some aliases we can use to make the shell commands in this notebook easier to read.\n",
    "mongodump = cfg.mongodump_path\n",
    "mongorestore = cfg.mongorestore_path\n",
    "\n",
    "# Perform a sanity test of the application paths.\n",
    "!{mongodump} --version\n",
    "!{mongorestore} --version"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "68245d2b",
   "metadata": {},
   "source": [
    "### Create MongoDB clients\n",
    "\n",
    "Create MongoDB clients you can use to access the \"origin\" and \"transformer\" MongoDB servers."
   ]
  },
  {
   "cell_type": "code",
   "id": "8e95f559",
   "metadata": {},
   "source": [
    "# Mongo client for \"origin\" MongoDB server.\n",
    "origin_mongo_client = pymongo.MongoClient(host=cfg.origin_mongo_server_uri, directConnection=True)\n",
    "\n",
    "# Mongo client for \"transformer\" MongoDB server.\n",
    "transformer_mongo_client = pymongo.MongoClient(host=cfg.transformer_mongo_server_uri)\n",
    "\n",
    "# Perform sanity tests of those MongoDB clients' abilities to access their respective MongoDB servers.\n",
    "with pymongo.timeout(3):\n",
    "    # Display the MongoDB server version (running on the \"origin\" Mongo server).\n",
    "    print(\"Origin Mongo server version:      \" + origin_mongo_client.server_info()[\"version\"])\n",
    "\n",
    "    # Sanity test: Ensure the origin database exists.\n",
    "    assert \"nmdc\" in origin_mongo_client.list_database_names(), \"Origin database does not exist.\"\n",
    "\n",
    "    # Display the MongoDB server version (running on the \"transformer\" Mongo server).\n",
    "    print(\"Transformer Mongo server version: \" + transformer_mongo_client.server_info()[\"version\"])\n",
    "\n",
    "    # Sanity test: Ensure the transformation database does not exist.\n",
    "    assert \"nmdc\" not in transformer_mongo_client.list_database_names(), \"Transformation database already exists.\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc387abc62686091",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create a bookkeeper\n",
    "\n",
    "Create a `Bookkeeper` that can be used to document migration events in the \"origin\" server."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c982eb0c04e606d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bookkeeper = Bookkeeper(mongo_client=origin_mongo_client)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3975ac24",
   "metadata": {},
   "source": [
    "### Create JSON Schema validator\n",
    "\n",
    "In this step, you'll create a JSON Schema validator for the NMDC Schema."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e2dbb92",
   "metadata": {},
   "source": [
    "nmdc_jsonschema: dict = get_nmdc_jsonschema_dict()\n",
    "nmdc_jsonschema_validator = Draft7Validator(nmdc_jsonschema)  # Is there a newer validator class available; e.g. draft 2019?\n",
    "\n",
    "# Perform sanity tests of the NMDC Schema dictionary and the JSON Schema validator.\n",
    "# Reference: https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/protocols/#jsonschema.protocols.Validator.check_schema\n",
    "print(\"NMDC Schema title:   \" + nmdc_jsonschema[\"title\"])\n",
    "print(\"NMDC Schema version: \" + nmdc_jsonschema[\"version\"])\n",
    "\n",
    "nmdc_jsonschema_validator.check_schema(nmdc_jsonschema)  # raises exception if schema is invalid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd4994a0",
   "metadata": {},
   "source": [
    "### Dump collections from the \"origin\" MongoDB server\n",
    "\n",
    "Use `mongodump` to dump the collections involved in this migration **from** the \"origin\" MongoDB server **into** a local directory.\n",
    "\n",
    "> Since `mongodump` doesn't provide a CLI option we can use to specify the collections we _want_ the dump to include, we use multiple occurrences of the `--excludeCollection` CLI option to exclude each collection we do _not_ want the dump to include. The end result is the same—there's just that extra step involved."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf8fa1ca",
   "metadata": {},
   "source": [
    "# Build a string containing zero or more `--excludeCollection=\"...\"` options, which can be included in a `mongodump` command.\n",
    "all_collection_names: list[str] = origin_mongo_client[\"nmdc\"].list_collection_names()\n",
    "non_agenda_collection_names = [name for name in all_collection_names if name not in COLLECTION_NAMES]\n",
    "exclusion_options = [f\"--excludeCollection='{name}'\" for name in non_agenda_collection_names]\n",
    "exclusion_options_str = \" \".join(exclusion_options)  # separates each option with a space\n",
    "print(exclusion_options_str)\n",
    "\n",
    "# Dump the not-excluded collections from the \"origin\" database.\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{cfg.origin_dump_folder_path}\" \\\n",
    "  {exclusion_options_str}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3e3c9c4",
   "metadata": {},
   "source": [
    "### Load the dumped collections into the \"transformer\" MongoDB server\n",
    "\n",
    "Use `mongorestore` to load the dumped collections **from** the local directory **into** the \"transformer\" MongoDB server.\n",
    "\n",
    "> Since it's possible that the dump included extra collections (due to someone having created a collection between the time you generated the `--excludeCollection` CLI options and the time you ran `mongodump` above), we will use the `--nsInclude` CLI option to indicate which specific collections—from the dump—we want to load into the \"transformer\" database.\n",
    "\n",
    "> Note: This step typically takes 3 minutes (on a MacBook Pro M1, when running MongoDB in a Docker container)."
   ]
  },
  {
   "cell_type": "code",
   "id": "418571c5",
   "metadata": {},
   "source": [
    "# Build a string containing zero or more `--nsInclude=\"...\"` options, which can be included in a `mongorestore` command.\n",
    "inclusion_options = [f\"--nsInclude='nmdc.{name}'\" for name in COLLECTION_NAMES]\n",
    "inclusion_options_str = \" \".join(inclusion_options)  # separates each option with a space\n",
    "print(inclusion_options_str)\n",
    "\n",
    "# Restore the dumped collections to the \"transformer\" MongoDB server.\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --drop \\\n",
    "  --preserveUUID \\\n",
    "  --stopOnError \\\n",
    "  --dir=\"{cfg.origin_dump_folder_path}\" \\\n",
    "  {inclusion_options_str}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c090068",
   "metadata": {},
   "source": [
    "### Transform the collections within the \"transformer\" MongoDB server\n",
    "\n",
    "Use the migrator to transform the collections in the \"transformer\" database.\n",
    "\n",
    "> Reminder: The database transformation functions are defined in the `nmdc-schema` Python package installed earlier.\n",
    "\n",
    "> Reminder: The \"origin\" database is **not** affected by this step."
   ]
  },
  {
   "cell_type": "code",
   "id": "05869340",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# Instantiate a MongoAdapter bound to the \"transformer\" database.\n",
    "adapter = MongoAdapter(\n",
    "    database=transformer_mongo_client[\"nmdc\"],\n",
    "    on_collection_created=lambda name: print(f'Created collection \"{name}\"'),\n",
    "    on_collection_renamed=lambda old_name, name: print(f'Renamed collection \"{old_name}\" to \"{name}\"'),\n",
    "    on_collection_deleted=lambda name: print(f'Deleted collection \"{name}\"'),\n",
    ")\n",
    "\n",
    "# Instantiate Migrators bound to that adapter.\n",
    "migrator1 = M1(adapter=adapter)\n",
    "migrator2 = M2(adapter=adapter)\n",
    "migrator3 = M3(adapter=adapter)\n",
    "migrator4 = M4(adapter=adapter)\n",
    "migrator5 = M5(adapter=adapter)\n",
    "migrator6 = M6(adapter=adapter)\n",
    "migrator7 = M7(adapter=adapter)\n",
    "migrator8 = M8(adapter=adapter)\n",
    "migrator9 = M9(adapter=adapter)\n",
    "migrator10 = M10(adapter=adapter)\n",
    "migrator11 = M11(adapter=adapter)\n",
    "migrator12 = M12(adapter=adapter)\n",
    "\n",
    "# Execute the Migrator's `upgrade` method to perform the migration.\n",
    "print(f\"[{get_timestamp()}] Calling migrator1.upgrade()\")\n",
    "migrator1.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator2.upgrade()\")\n",
    "migrator2.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator3.upgrade()\")\n",
    "migrator3.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator4.upgrade()\")\n",
    "migrator4.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator5.upgrade()\")\n",
    "migrator5.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator6.upgrade()\")\n",
    "migrator6.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator7.upgrade()\")\n",
    "migrator7.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator8.upgrade()\")\n",
    "migrator8.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator9.upgrade()\")\n",
    "migrator9.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator10.upgrade()\")\n",
    "migrator10.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator11.upgrade()\")\n",
    "migrator11.upgrade()\n",
    "print(f\"[{get_timestamp()}] Calling migrator12.upgrade()\")\n",
    "migrator12.upgrade()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3edf77c7",
   "metadata": {},
   "source": [
    "### Validate the transformed documents\n",
    "\n",
    "Now that we have transformed the database, validate each document in each collection in the \"transformer\" MongoDB server.\n",
    "\n",
    "> Reference: https://github.com/microbiomedata/nmdc-runtime/blob/main/metadata-translation/src/bin/validate_json.py"
   ]
  },
  {
   "cell_type": "code",
   "id": "db6e432d",
   "metadata": {},
   "source": [
    "for collection_name in COLLECTION_NAMES:\n",
    "    collection = transformer_mongo_client[\"nmdc\"][collection_name]\n",
    "    for document in collection.find():\n",
    "        # Validate the transformed document.\n",
    "        #\n",
    "        # Reference: https://github.com/microbiomedata/nmdc-schema/blob/main/src/docs/schema-validation.md\n",
    "        #\n",
    "        # Note: Dictionaries originating as Mongo documents include a Mongo-generated key named `_id`. However,\n",
    "        #       the NMDC Schema does not describe that key and, indeed, data validators consider dictionaries\n",
    "        #       containing that key to be invalid with respect to the NMDC Schema. So, here, we validate a\n",
    "        #       copy (i.e. a shallow copy) of the document that lacks that specific key.\n",
    "        #\n",
    "        # Note: `root_to_validate` is a dictionary having the shape: { \"some_collection_name\": [ some_document ] }\n",
    "        #       Reference: https://docs.python.org/3/library/stdtypes.html#dict (see the \"type constructor\" section)\n",
    "        #\n",
    "        document_without_underscore_id_key = {key: value for key, value in document.items() if key != \"_id\"}\n",
    "        root_to_validate = dict([(collection_name, [document_without_underscore_id_key])])\n",
    "        nmdc_jsonschema_validator.validate(root_to_validate)  # raises exception if invalid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "997fcb281d9d3222",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Indicate that the migration is underway\n",
    "\n",
    "Add an entry to the migration log collection to indicate that this migration has started."
   ]
  },
  {
   "cell_type": "code",
   "id": "fcafd862e1becb98",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bookkeeper.record_migration_event(migrator=migrator1, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator2, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator3, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator4, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator5, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator6, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator7, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator8, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator9, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator10, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator11, event=MigrationEvent.MIGRATION_STARTED)\n",
    "bookkeeper.record_migration_event(migrator=migrator12, event=MigrationEvent.MIGRATION_STARTED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e0c8891",
   "metadata": {},
   "source": [
    "### Dump the collections from the \"transformer\" MongoDB server\n",
    "\n",
    "Now that the collections have been transformed and validated, dump them **from** the \"transformer\" MongoDB server **into** a local directory."
   ]
  },
  {
   "cell_type": "code",
   "id": "ca49f61a",
   "metadata": {},
   "source": [
    "# Dump the database from the \"transformer\" MongoDB server.\n",
    "!{mongodump} \\\n",
    "  --config=\"{cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{cfg.transformer_dump_folder_path}\" \\\n",
    "  {exclusion_options_str}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d84bdc11",
   "metadata": {},
   "source": [
    "### Load the collections into the \"origin\" MongoDB server\n",
    "\n",
    "Load the transformed collections into the \"origin\" MongoDB server, **replacing** the collections there that have the same names.\n",
    "\n",
    "> Note: If the migration involved renaming or deleting a collection, the collection having the original name will continue to exist in the \"origin\" database until someone deletes it manually."
   ]
  },
  {
   "cell_type": "code",
   "id": "1dfbcf0a",
   "metadata": {},
   "source": [
    "# Replace the same-named collection(s) on the origin server, with the transformed one(s).\n",
    "!{mongorestore} \\\n",
    "  --config=\"{cfg.origin_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --verbose \\\n",
    "  --dir=\"{cfg.transformer_dump_folder_path}\" \\\n",
    "  --drop \\\n",
    "  --stopOnError \\\n",
    "  --preserveUUID \\\n",
    "  {inclusion_options_str}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca5ee89a79148499",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Indicate that the migration is complete\n",
    "\n",
    "Add an entry to the migration log collection to indicate that this migration is complete."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1eaa6c92789c4f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bookkeeper.record_migration_event(migrator=migrator1, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator2, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator3, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator4, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator5, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator6, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator7, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator8, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator9, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator10, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator11, event=MigrationEvent.MIGRATION_COMPLETED)\n",
    "bookkeeper.record_migration_event(migrator=migrator12, event=MigrationEvent.MIGRATION_COMPLETED)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
