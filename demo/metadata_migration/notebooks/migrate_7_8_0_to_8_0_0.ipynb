{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate from `v7.8.0` to `v8.0.0`\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Start a MongoDB server on your local machine (or in a Docker container) and ensure it does **not** contain a database named `nmdc`.\n",
    "2. Create and populate a **notebook configuration file** named `.notebook.env`.\n",
    "    1. You can use the `.notebook.env.example` file as a template:\n",
    "       ```shell\n",
    "       cp .notebook.env.example .notebook.env\n",
    "       ```\n",
    "3. Create and populate **Mongo configuration files** for connecting to the origin and transformer Mongo servers.\n",
    "    1. You can use the `.mongo.yaml.example` file as a template:\n",
    "       ```shell\n",
    "       cp .mongo.yaml.example .mongo.origin.yaml\n",
    "       cp .mongo.yaml.example .mongo.transformer.yaml\n",
    "       ```\n",
    "       > When populating the file for the origin Mongo server, use root credentials since this notebook will be manipulating user roles on that server.\n",
    "4. Run the cells in this notebook in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the third-party Python packages upon which this notebook (and `./helpers.py`) depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /Users/EECavanna/mambaforge/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/EECavanna/mambaforge/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/EECavanna/mambaforge/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/EECavanna/mambaforge/lib/python3.10/site-packages (from pymongo->-r requirements.txt (line 1)) (2.4.2)\n",
      "annotated-types==0.5.0\n",
      "antlr4-python3-runtime==4.9.3\n",
      "appnope @ file:///home/conda/feedstock_root/build_artifacts/appnope_1649077682618/work\n",
      "arrow==1.2.3\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work\n",
      "attrs==23.1.0\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1687772187254/work\n",
      "build==0.10.0\n",
      "certifi==2023.5.7\n",
      "cffi @ file:///Users/runner/miniforge3/conda-bld/cffi_1671179893800/work\n",
      "CFGraph==0.2.1\n",
      "chardet==5.2.0\n",
      "charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1678108872112/work\n",
      "click==8.1.7\n",
      "click-log==0.4.0\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\n",
      "comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1679481329611/work\n",
      "conda==23.1.0\n",
      "conda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1669907009957/work\n",
      "conda_package_streaming @ file:///home/conda/feedstock_root/build_artifacts/conda-package-streaming_1685101166527/work\n",
      "cryptography @ file:///Users/runner/miniforge3/conda-bld/cryptography-split_1685659597707/work\n",
      "curies==0.6.1\n",
      "debugpy @ file:///Users/runner/miniforge3/conda-bld/debugpy_1680755676047/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "Deprecated==1.2.14\n",
      "dnspython==2.4.2\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.1.3\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work\n",
      "fqdn==1.5.1\n",
      "graphviz==0.20.1\n",
      "greenlet==2.0.1\n",
      "hbreader==0.9.1\n",
      "idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work\n",
      "importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1687138353019/work\n",
      "iniconfig==2.0.0\n",
      "ipykernel @ file:///Users/runner/miniforge3/conda-bld/ipykernel_1687739158450/work\n",
      "ipython @ file:///Users/runner/miniforge3/conda-bld/ipython_1685727999785/work\n",
      "isodate==0.6.1\n",
      "isoduration==20.11.0\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1669134318875/work\n",
      "Jinja2==3.1.2\n",
      "json-flattener==0.1.9\n",
      "jsonasobj==1.3.1\n",
      "jsonasobj2==1.0.4\n",
      "jsonpatch==1.33\n",
      "jsonpath-ng==1.5.3\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.19.0\n",
      "jsonschema-specifications==2023.7.1\n",
      "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1687700988094/work\n",
      "jupyter_core @ file:///Users/runner/miniforge3/conda-bld/jupyter_core_1686775679566/work\n",
      "libmambapy @ file:///Users/runner/miniforge3/conda-bld/mamba-split_1680791351028/work/libmambapy\n",
      "linkml==1.5.7\n",
      "linkml-dataops==0.1.0\n",
      "linkml-runtime==1.5.6\n",
      "mamba @ file:///Users/runner/miniforge3/conda-bld/mamba-split_1680791351028/work/mamba\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work\n",
      "nmdc_schema==7.8.0\n",
      "openpyxl==3.1.2\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work\n",
      "parse==1.19.1\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "pip-tools==7.3.0\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1687705014305/work\n",
      "pluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1687776318736/work\n",
      "ply==3.11\n",
      "prefixcommons==0.1.12\n",
      "prefixmaps==0.1.5\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1677600924538/work\n",
      "psutil @ file:///Users/runner/miniforge3/conda-bld/psutil_1681775207008/work\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "pycosat @ file:///Users/runner/miniforge3/conda-bld/pycosat_1666836623787/work\n",
      "pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work\n",
      "pydantic==2.3.0\n",
      "pydantic_core==2.6.3\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1681904169130/work\n",
      "PyJSG==0.11.10\n",
      "pymongo==4.5.0\n",
      "pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1685514481738/work\n",
      "pyparsing==3.1.1\n",
      "pyproject_hooks==1.0.0\n",
      "PyShEx==0.8.1\n",
      "PyShExC==0.9.1\n",
      "PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work\n",
      "pytest==7.4.2\n",
      "pytest-logging==2015.11.4\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\n",
      "python-dotenv==1.0.0\n",
      "PyTrie==0.4.0\n",
      "PyYAML==6.0.1\n",
      "pyzmq @ file:///Users/runner/miniforge3/conda-bld/pyzmq_1685519412652/work\n",
      "rdflib==7.0.0\n",
      "rdflib-jsonld==0.6.1\n",
      "rdflib-shim==1.0.3\n",
      "referencing==0.30.2\n",
      "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3987==1.3.8\n",
      "rpds-py==0.10.2\n",
      "ruamel.yaml @ file:///Users/runner/miniforge3/conda-bld/ruamel.yaml_1686993977949/work\n",
      "ruamel.yaml.clib @ file:///Users/runner/miniforge3/conda-bld/ruamel.yaml.clib_1670412881259/work\n",
      "ShExJSG==0.8.2\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\n",
      "sortedcontainers==2.4.0\n",
      "sparqlslurper==0.5.1\n",
      "SPARQLWrapper==2.0.0\n",
      "SQLAlchemy==2.0.20\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\n",
      "tomli==2.0.1\n",
      "toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work\n",
      "tornado @ file:///Users/runner/miniforge3/conda-bld/tornado_1684150425472/work\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1677948868469/work\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work\n",
      "typing_extensions==4.7.1\n",
      "uri-template==1.3.0\n",
      "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1686156552494/work\n",
      "watchdog==3.0.0\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work\n",
      "webcolors==1.13\n",
      "wrapt==1.15.0\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1677313463193/work\n",
      "zstandard==0.19.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Python objects upon which this notebook depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library packages:\n",
    "from pprint import pformat\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Third-party packages:\n",
    "import pymongo\n",
    "\n",
    "# First-party packages:\n",
    "from helpers import Config as Cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MongoDB clients\n",
    "\n",
    "Create MongoDB clients you can use to access the \"origin\" MongoDB server (i.e. the one containing the database you want to migrate) and the \"transformer\" MongoDB server (i.e. the one you want to use to perform the data transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB client for origin MongoDB server.\n",
    "origin_mongo_client = pymongo.MongoClient(host=Cfg.origin_mongo_server_uri, directConnection=True)\n",
    "\n",
    "# MongoDB client for transformer MongoDB server.\n",
    "transformer_mongo_client = pymongo.MongoClient(host=Cfg.transformer_mongo_server_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable writing to the origin MongoDB database\n",
    "\n",
    "To disable writing to the database, I will eventually set all users' roles (except the admin user) to `read` (i.e. read-only) with respect to the database. Before I carry out that plan, though, I will store the original users for future reference (so I can restore their original roles later).\n",
    "\n",
    "Note: `pymongo` does not offer [`db.getUsers()`](https://www.mongodb.com/docs/manual/reference/method/db.getUsers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: dict = origin_mongo_client[\"admin\"].command(\"usersInfo\")\n",
    "users_initial = result[\"users\"]\n",
    "\n",
    "# Create temporary file in the notebook's folder, containing the initial users.\n",
    "users_file = NamedTemporaryFile(delete=False, dir=str(Path.cwd()), prefix=\"tmp.origin_users_initial.\")\n",
    "users_file.write(bytes(pformat(users_initial), \"utf-8\"))\n",
    "users_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've stored their original roles, I'll convert every `readWrite` role (with respect to the `nmdc` database) into just plain `read`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users_initial:\n",
    "\n",
    "    break  # Abort! TODO: Remove me when I'm ready to run this notebook for real.\n",
    "\n",
    "    if any((role[\"db\"] == \"nmdc\") for role in user[\"roles\"]):\n",
    "        origin_mongo_client[\"admin\"].command(\"grantRolesToUser\", user[\"user\"], roles=[{ \"role\": \"read\", \"db\": \"nmdc\" }])\n",
    "        origin_mongo_client[\"admin\"].command(\"revokeRolesFromUser\", user[\"user\"], roles=[{ \"role\": \"readWrite\", \"db\": \"nmdc\" }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dump the necessary collections from the origin database\n",
    "\n",
    "In this case, I'll dump the `extraction_set` collection only.\n",
    "\n",
    "References:\n",
    "- https://www.mongodb.com/docs/database-tools/mongodump/\n",
    "- https://www.mongodb.com/docs/database-tools/mongodump/#std-option-mongodump.--config (`--config` option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the database from the origin MongoDB server.\n",
    "!{mongodump} \\\n",
    "  --config=\"{Cfg.origin_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --collection=\"extraction_set\" \\\n",
    "  --out=\"{Cfg.origin_dump_folder_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the database into the transformer MongoDB server\n",
    "\n",
    "References:\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--config (`--config` option)\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--drop (`--drop` to drop the existing collection)\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--preserveUUID (`--preserveUUID` to use the existing UUIDs from the dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the database to the transformer MongoDB server.\n",
    "!{mongorestore} \\\n",
    "  --config=\"{Cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --drop --preserveUUID \\\n",
    "  --dir=\"{Cfg.origin_dump_folder_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the database\n",
    "\n",
    "Now that the transformer database contains a copy of the subject database, we can transform it there.\n",
    "\n",
    "Source: https://github.com/microbiomedata/nmdc-schema/blob/7802f295cfc80d056f9c73c79636802926be40ee/nmdc_schema/migration_recursion.py#L38\n",
    "- Note: source will need to be changed when the above link is merged into main.\n",
    "- Note: \"NEXT\" in the function's name will need to be changed to correct version number\n",
    "- removed logger calls\n",
    "- removed commented out line\n",
    "\n",
    "References:\n",
    "- https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.replace_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <copy_pasted_snippet from=\"https://github.com/microbiomedata/nmdc-schema/blob/7802f295cfc80d056f9c73c79636802926be40ee/nmdc_schema/migration_recursion.py#L38\">\n",
    "\n",
    "def migrate_extractions_7_8_0_to_8_0_0(retrieved_extraction):\n",
    "\n",
    "    if \"sample_mass\" in retrieved_extraction:\n",
    "        retrieved_extraction['input_mass'] = retrieved_extraction.pop('sample_mass')\n",
    "        \n",
    "    return retrieved_extraction\n",
    "\n",
    "# </copy_pasted_snippet>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a transformed version of each extraction in the transformer database.\n",
    "transformed_extractions = []\n",
    "for extraction in transformer_mongo_client[\"nmdc\"][\"extraction_set\"].find():\n",
    "    print(extraction)\n",
    "    transformed_extraction = migrate_extractions_7_8_0_to_8_0_0(extraction)\n",
    "    transformed_extractions.append(transformed_extraction)\n",
    "    print(transformed_extraction)\n",
    "\n",
    "# Replace the original versions with the transformed versions of themselves (in the transformer database).\n",
    "for transformed_extraction in transformed_extractions:\n",
    "    transformer_mongo_client[\"nmdc\"][\"extraction_set\"].replace_one({\"id\": {\"$eq\": transformed_extraction[\"id\"]}}, transformed_extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the transformed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the transformed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the database from the transformer MongoDB server.\n",
    "!{mongodump} \\\n",
    "  --config=\"{Cfg.transformer_mongo_config_file_path}\" \\\n",
    "  --db=\"nmdc\" \\\n",
    "  --gzip \\\n",
    "  --out=\"{Cfg.transformer_dump_folder_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the transformed data into the origin MongoDB server\n",
    "\n",
    "In the case of this migration, given how focused the transformation was (i.e. only the `extraction_set` collection was affected), I will restore **only** the `extraction_set` collection to the origin server.\n",
    "\n",
    "References:\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--nsInclude (`--nsInclude` to specify which collections to affect)\n",
    "- https://www.mongodb.com/docs/database-tools/mongorestore/#std-option-mongorestore.--dryRun (`--dryRun` can be used to preview the outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original `extraction_set` collection from the origin server,\n",
    "# and restore the transformed `extraction_set` collection into its place.\n",
    "!{mongorestore} \\\n",
    "  --config=\"{Cfg.origin_mongo_config_file_path}\" \\\n",
    "  --gzip \\\n",
    "  --verbose \\\n",
    "  --dir=\"{Cfg.transformer_dump_folder_path}\" \\\n",
    "  --nsInclude=\"nmdc.extraction_set\" \\\n",
    "  --drop --preserveUUID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've restored the database, I'll restore the original user roles (with respect to the `nmdc` database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users_initial:\n",
    "\n",
    "    break  # Abort! TODO: Remove me when I'm ready to run this notebook for real.\n",
    "\n",
    "    if any((role[\"db\"] == \"nmdc\" and role[\"role\"] == \"readWrite\") for role in user[\"roles\"]):\n",
    "        origin_mongo_client[\"admin\"].command(\"grantRolesToUser\", user[\"user\"], roles=[{ \"role\": \"readWrite\", \"db\": \"nmdc\" }])\n",
    "        origin_mongo_client[\"admin\"].command(\"revokeRolesFromUser\", user[\"user\"], roles=[{ \"role\": \"read\", \"db\": \"nmdc\" }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About db.fsyncLock() and db.fsyncUnlock()\n",
    "\n",
    "I chose not to use `db.fsyncLock()`/`db.fsyncUnlock()` as the method of disabling/re-enabling write access, because I want to be able to `mongorestore` a database while write access is still disabled. `db.fsyncLock()` would have disabled write access at the `mongod` level, preventing database-level write operations (but still allowing a system administrator to \"backup\" database **files** via `cp`, `scp`, `tar`, etc.\n",
    "\n",
    "Reference: https://www.mongodb.com/docs/manual/reference/method/db.fsyncLock/#mongodb-method-db.fsyncLock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "You may want to manually delete the `.tmp.*` files that this notebook created in its folder. Some of them contain MongoDB passwords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
